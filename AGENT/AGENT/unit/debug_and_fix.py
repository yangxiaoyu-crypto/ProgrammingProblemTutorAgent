# # import uuid
# # import re
# # from coper.LLM import LLM
# # from coper.basic_ops import Mul
# # from core.Context import Context
# # from coper.Service import Service
# # import logging
# # log_filename = f"judge_error_type.log"
# # logging.basicConfig(
# #     level=logging.INFO,
# #     format='%(asctime)s [%(levelname)s] - %(message)s',
# #     handlers=[
# #         logging.FileHandler(log_filename, encoding='utf-8'),
# #         logging.StreamHandler()  # åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°
# #     ]
# # )
# # SUBMISSION_LANGUAGE = "C++14"
# # def debug_and_fix(problem_details: dict, buggy_code: str, failed_case_input: str, expected_output: str,
# #                            actual_output: str, language: str, model: str, user_hint: str = None):
# #     """è¯·æ±‚LLMåˆ†æå¹¶ä¿®å¤bug"""
# #     logging.info("ğŸ¤– æ­£åœ¨è¯·æ±‚ AI åˆ†æå¹¶ä¿®å¤ä»£ç ...")

# #     prompt = f"""
# # [SYSTEM]
# # ä½ æ˜¯ä¸€ä½é¡¶çº§çš„è½¯ä»¶è°ƒè¯•ä¸“å®¶ï¼Œç²¾é€š {language} è¯­è¨€ã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ†æä¸€æ®µæœ‰é”™è¯¯çš„ä»£ç ï¼Œå¹¶æ ¹æ®ä¸€ä¸ªå¯¼è‡´å¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹æ¥ä¿®å¤å®ƒã€‚

# # [USER]
# # è¯·åˆ†æä»¥ä¸‹æœ‰é—®é¢˜çš„ä»£ç ã€‚å®ƒåœ¨å¤„ç†ç»™å®šçš„è¾“å…¥æ—¶ï¼Œæœªèƒ½äº§ç”Ÿé¢„æœŸçš„è¾“å‡ºã€‚

# # --- é—®é¢˜æè¿° ---
# # {problem_details['full_markdown_description']}
# # --- é—®é¢˜æè¿°ç»“æŸ ---

# # --- å¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹ ---
# # è¾“å…¥ (Input):
# # {failed_case_input}
# # é¢„æœŸçš„è¾“å‡º (Expected Output):
# # {expected_output}
# # å®é™…çš„é”™è¯¯è¾“å‡º (Actual Output):
# # {actual_output}
# # --- å¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹ç»“æŸ ---
# # """
# #     if user_hint:
# #         prompt += f"""
# # --- äººç±»å¼€å‘è€…çš„æç¤º ---
# # {user_hint}
# # --- æç¤ºç»“æŸ ---
# # """
# #     prompt += f"""
# # --- æœ‰é—®é¢˜çš„ä»£ç  ---
# # ```{language.lower()}
# # {buggy_code}
# # --- æœ‰é—®é¢˜çš„ä»£ç ç»“æŸ ---
# # è¯·åœ¨'thought'éƒ¨åˆ†è¯¦ç»†åˆ†æé”™è¯¯çš„åŸå› ï¼Œç„¶ååœ¨'code'éƒ¨åˆ†æä¾›å®Œæ•´çš„ã€ä¿®æ­£åçš„ä»£ç ã€‚
# # """
# #     try:
# #         llm = LLM(model)
# #         response = llm(prompt, structured_output=SolutionOutput.model_json_schema()).result()
# #         structured_data = response.get("structured_output")
# #         if structured_data and structured_data.get("code"):
# #             logging.info("âœ… AI å·²ç”Ÿæˆä¿®æ­£åçš„ä»£ç ã€‚")
# #             return structured_data.get("code")
# #         logging.error("âŒ AI æœªèƒ½ç”Ÿæˆæœ‰æ•ˆçš„ä¿®æ­£ä»£ç ã€‚å°†è¿”å›åŸå§‹ä»£ç ã€‚")
# #         return buggy_code
# #     except Exception as e:
# #         logging.critical(f"âŒ è¯·æ±‚ AI ä¿®å¤ä»£ç æ—¶å‘ç”Ÿé”™è¯¯: {e}")
# #         return buggy_code
# import uuid
# import re
# import logging
# import traceback
# import json
# from coper.LLM import LLM
# from coper.basic_ops import Mul
# from core.Context import Context
# from coper.Service import Service

# log_filename = f"judge_error_type.log"
# logging.basicConfig(
#     level=logging.INFO,
#     format='%(asctime)s [%(levelname)s] - %(message)s',
#     handlers=[
#         logging.FileHandler(log_filename, encoding='utf-8'),
#         logging.StreamHandler()  # åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°
#     ]
# )

# SUBMISSION_LANGUAGE = "C++14"

# def debug_and_fix(problem_details: dict, buggy_code: str, failed_case_input: str, expected_output: str,
#                   actual_output: str, language: str, model: str, user_hint: str = None):
#     """è¯·æ±‚LLMåˆ†æå¹¶ä¿®å¤bug"""
#     logging.info("ğŸ¤– æ­£åœ¨è¯·æ±‚ AI åˆ†æå¹¶ä¿®å¤ä»£ç ...")

#     # åˆ›å»ºåŸºç¡€æç¤º
#     prompt = f"""
# [SYSTEM]
# ä½ æ˜¯ä¸€ä½é¡¶çº§çš„è½¯ä»¶è°ƒè¯•ä¸“å®¶ï¼Œç²¾é€š {language} è¯­è¨€ã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ†æä¸€æ®µæœ‰é”™è¯¯çš„ä»£ç ï¼Œå¹¶æ ¹æ®ä¸€ä¸ªå¯¼è‡´å¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹æ¥ä¿®å¤å®ƒã€‚

# [USER]
# è¯·åˆ†æä»¥ä¸‹æœ‰é—®é¢˜çš„ä»£ç ã€‚å®ƒåœ¨å¤„ç†ç»™å®šçš„è¾“å…¥æ—¶ï¼Œæœªèƒ½äº§ç”Ÿé¢„æœŸçš„è¾“å‡ºã€‚

# --- é—®é¢˜æè¿° ---
# {problem_details['full_markdown_description']}
# --- é—®é¢˜æè¿°ç»“æŸ ---

# --- å¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹ ---
# è¾“å…¥ (Input):
# {failed_case_input}
# é¢„æœŸçš„è¾“å‡º (Expected Output):
# {expected_output}
# å®é™…çš„é”™è¯¯è¾“å‡º (Actual Output):
# {actual_output}
# --- å¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹ç»“æŸ ---
# """
#     if user_hint:
#         prompt += f"""
# --- äººç±»å¼€å‘è€…çš„æç¤º ---
# {user_hint}
# --- æç¤ºç»“æŸ ---
# """
#     prompt += f"""
# --- æœ‰é—®é¢˜çš„ä»£ç  ---
# {language.lower()}
# {buggy_code}
# --- æœ‰é—®é¢˜çš„ä»£ç ç»“æŸ ---
# """
    
#     # ç»“æ„åŒ–è¾“å‡ºæç¤º
#     structured_prompt = prompt + """
# è¯·åœ¨'thought'éƒ¨åˆ†è¯¦ç»†åˆ†æé”™è¯¯çš„åŸå› ï¼Œç„¶ååœ¨'code'éƒ¨åˆ†æä¾›å®Œæ•´çš„ã€ä¿®æ­£åçš„ä»£ç ã€‚
# """
    
#     # è‡ªç„¶è¯­è¨€æç¤º
#     natural_prompt = prompt + """
# è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š
# 1. é¦–å…ˆåœ¨'åˆ†æ'éƒ¨åˆ†è¯¦ç»†è¯´æ˜é”™è¯¯åŸå› 
# 2. ç„¶ååœ¨'ä¿®å¤åçš„ä»£ç 'éƒ¨åˆ†æä¾›å®Œæ•´çš„ã€ä¿®æ­£åçš„ä»£ç 
# 3. æœ€ååœ¨'è§£é‡Š'éƒ¨åˆ†ç®€è¦è¯´æ˜ä¿®å¤çš„å…³é”®ç‚¹

# è¯·å°†ä¿®å¤åçš„ä»£ç æ”¾åœ¨å•ç‹¬çš„ä»£ç å—ä¸­ï¼Œä½¿ç”¨ä»¥ä¸‹æ ¼å¼ï¼š
# {language.lower()}
# // ä¿®å¤åçš„ä»£ç 
# ...

# """
    
#     try:
#         llm = LLM(model)
#         fixed_code = None
        
#         # é¦–å…ˆå°è¯•ç»“æ„åŒ–è¾“å‡ºæ–¹å¼
#         try:
#             # å®šä¹‰æœŸæœ›çš„ç»“æ„åŒ–è¾“å‡ºæ ¼å¼
#             structured_output_schema = {
#                 "type": "object",
#                 "properties": {
#                     "thought": {"type": "string"},
#                     "code": {"type": "string"}
#                 },
#                 "required": ["thought", "code"]
#             }
            
#             response = llm(structured_prompt, structured_output=structured_output_schema).result()
#             structured_data = response.get("structured_output")
            
#             if structured_data and structured_data.get("code"):
#                 fixed_code = structured_data.get("code")
#                 logging.info("âœ… AI å·²é€šè¿‡ç»“æ„åŒ–è¾“å‡ºç”Ÿæˆä¿®æ­£åçš„ä»£ç ã€‚")
        
#         except Exception as e:
#             logging.warning(f"ç»“æ„åŒ–è¾“å‡ºå¤±è´¥: {e}, å°è¯•è‡ªç„¶è¯­è¨€æ–¹å¼...")
        
#         # å¦‚æœç»“æ„åŒ–è¾“å‡ºå¤±è´¥ï¼Œå°è¯•è‡ªç„¶è¯­è¨€æ–¹å¼
#         if not fixed_code:
#             response = llm(natural_prompt).result()
#             content = response.get("content", "")
            
#             logging.debug(f"AI å“åº”å†…å®¹:\n{content}")
            
#             # å°è¯•ä»å“åº”ä¸­æå–ä¿®å¤åçš„ä»£ç 
#             code_match = re.search(rf"`{language.lower()}\s*(.*?)\s*`", content, re.DOTALL)
#             if code_match:
#                 fixed_code = code_match.group(1).strip()
#                 logging.info("âœ… AI å·²é€šè¿‡è‡ªç„¶è¯­è¨€ç”Ÿæˆä¿®æ­£åçš„ä»£ç ã€‚")
#             else:
#                 # å¦‚æœæ‰¾ä¸åˆ°ä»£ç å—ï¼Œå°è¯•æŸ¥æ‰¾å…¶ä»–æ ¼å¼çš„ä»£ç 
#                 logging.warning("âŒ æœªæ‰¾åˆ°æ ¼å¼åŒ–çš„ä»£ç å—ï¼Œå°è¯•æå–ä»£ç ç‰‡æ®µ...")
#                 code_match = re.search(r"ä¿®å¤åçš„ä»£ç [:ï¼š]?\s(.?)(?=è§£é‡Š|$)", content, re.DOTALL)
#                 if code_match:
#                     fixed_code = code_match.group(1).strip()
#                     logging.info("âœ… AI å·²ç”Ÿæˆä¿®æ­£åçš„ä»£ç ï¼ˆéæ ¼å¼åŒ–ï¼‰ã€‚")
        
#         # å¦‚æœä¸¤ç§æ–¹å¼éƒ½å¤±è´¥ï¼Œè¿”å›åŸå§‹ä»£ç 
#         if not fixed_code:
#             logging.error("âŒ AI æœªèƒ½ç”Ÿæˆæœ‰æ•ˆçš„ä¿®æ­£ä»£ç ã€‚å°†è¿”å›åŸå§‹ä»£ç ã€‚")
#             return buggy_code
        
#         return fixed_code
        
#     except Exception as e:
#         logging.critical(f"âŒ è¯·æ±‚ AI ä¿®å¤ä»£ç æ—¶å‘ç”Ÿé”™è¯¯: {e}")
#         logging.error(traceback.format_exc())
#         return buggy_code


import re
import logging
import traceback
import json
from coper.LLM import LLM
from coper.basic_ops import Mul
from core.Context import Context
from coper.Service import Service

log_filename = f"judge_error_type.log"
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] - %(message)s',
    handlers=[
        logging.FileHandler(log_filename, encoding='utf-8'),
        logging.StreamHandler()  # åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°
    ]
)

SUBMISSION_LANGUAGE = "C++14"

def debug_and_fix(problem_details: dict, buggy_code: str, failed_case_input: str, expected_output: str,
                  actual_output: str, language: str, model: str, user_hint: str = None):
    """è¯·æ±‚LLMåˆ†æå¹¶ä¿®å¤bug"""
    logging.info("ğŸ¤– æ­£åœ¨è¯·æ±‚ AI åˆ†æå¹¶ä¿®å¤ä»£ç ...")

    # åˆ›å»ºåŸºç¡€æç¤º
    prompt = f"""
[SYSTEM]
ä½ æ˜¯ä¸€ä½é¡¶çº§çš„è½¯ä»¶è°ƒè¯•ä¸“å®¶ï¼Œç²¾é€š {language} è¯­è¨€ã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ†æä¸€æ®µæœ‰é”™è¯¯çš„ä»£ç ï¼Œå¹¶æ ¹æ®ä¸€ä¸ªå¯¼è‡´å¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹æ¥ä¿®å¤å®ƒã€‚

[USER]
è¯·åˆ†æä»¥ä¸‹æœ‰é—®é¢˜çš„ä»£ç ã€‚å®ƒåœ¨å¤„ç†ç»™å®šçš„è¾“å…¥æ—¶ï¼Œæœªèƒ½äº§ç”Ÿé¢„æœŸçš„è¾“å‡ºã€‚

--- é—®é¢˜æè¿° ---
{problem_details['full_markdown_description']}
--- é—®é¢˜æè¿°ç»“æŸ ---

--- å¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹ ---
è¾“å…¥ (Input):
{failed_case_input}
é¢„æœŸçš„è¾“å‡º (Expected Output):
{expected_output}
å®é™…çš„é”™è¯¯è¾“å‡º (Actual Output):
{actual_output}
--- å¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹ç»“æŸ ---
"""
    if user_hint:
        prompt += f"""
--- äººç±»å¼€å‘è€…çš„æç¤º -f--
{user_hint}
--- æç¤ºç»“æŸ ---
"""
    prompt += f"""
--- æœ‰é—®é¢˜çš„ä»£ç  ---
{language.lower()}
{buggy_code}
--- æœ‰é—®é¢˜çš„ä»£ç ç»“æŸ ---
"""
    
    # ç»“æ„åŒ–è¾“å‡ºæç¤º
    structured_prompt = prompt + """
è¯·åœ¨'thought'éƒ¨åˆ†è¯¦ç»†åˆ†æé”™è¯¯çš„åŸå› ï¼Œç„¶ååœ¨'code'éƒ¨åˆ†æä¾›å®Œæ•´çš„ã€ä¿®æ­£åçš„ä»£ç ã€‚
"""
    
    # è‡ªç„¶è¯­è¨€æç¤º
    natural_prompt = prompt + f"""
ã€è¾“å‡ºæ ¼å¼è¦æ±‚ã€‘
1. å…ˆå†™ä¸€æ®µç®€çŸ­çš„åˆ†æï¼Œæ”¾åœ¨æ ‡ç­¾ <analysis> å’Œ </analysis> ä¹‹é—´ã€‚
2. ç´§æ¥ç€ç»™å‡º **å®Œæ•´** çš„ä¿®å¤åä»£ç ï¼Œæ”¾åœ¨ä¸€å¯¹ ```{language.lower()} å’Œ ``` ä¹‹é—´ï¼Œä¸­é—´ä¸è¦æ’å…¥ä»»ä½•è§£é‡Šæ–‡å­—ã€‚
3. æœ€åå¦‚æœ‰éœ€è¦ï¼Œå†ç”¨ <note> å’Œ </note> å†™ä¸€æ®µç®€çŸ­è¯´æ˜ã€‚

ç¤ºä¾‹æ¨¡æ¿ï¼š
<analysis>
è¿™é‡Œå†™é”™è¯¯åŸå› åˆ†æ
</analysis>

```{language.lower()}
// ä¿®å¤åçš„å®Œæ•´ä»£ç 
...
<note>
ï¼ˆå¯é€‰ï¼‰ä¿®å¤å…³é”®ç‚¹
</note>
ã€é‡è¦ã€‘åŠ¡å¿…ä¿æŒä¸Šè¿°æ ¼å¼ï¼Œå¦åˆ™è§£æä¼šå¤±è´¥ã€‚
"""
    
    try:
        llm = LLM(model)
        fixed_code = None
        
        # é¦–å…ˆå°è¯•ç»“æ„åŒ–è¾“å‡ºæ–¹å¼
        try:
            # å®šä¹‰æœŸæœ›çš„ç»“æ„åŒ–è¾“å‡ºæ ¼å¼
            structured_output_schema = {
                "type": "object",
                "properties": {
                    "thought": {"type": "string"},
                    "code": {"type": "string"}
                },
                "required": ["thought", "code"]
            }
            
            response = llm(structured_prompt, structured_output=structured_output_schema).result()
            structured_data = response.get("structured_output")
            
            if structured_data and structured_data.get("code"):
                fixed_code = structured_data.get("code")
                logging.info("âœ… AI å·²é€šè¿‡ç»“æ„åŒ–è¾“å‡ºç”Ÿæˆä¿®æ­£åçš„ä»£ç ã€‚")
        
        except Exception as e:
            logging.warning(f"ç»“æ„åŒ–è¾“å‡ºå¤±è´¥: {e}, å°è¯•è‡ªç„¶è¯­è¨€æ–¹å¼...")
        
        # å¦‚æœç»“æ„åŒ–è¾“å‡ºå¤±è´¥ï¼Œå°è¯•è‡ªç„¶è¯­è¨€æ–¹å¼
        if not fixed_code:
            response = llm(natural_prompt).result()
            content = response.get("content", "")
            logging.debug(f"AI å“åº”å†…å®¹:\n{content}")

            # ç»Ÿä¸€æå– ```cpp ... ``` æˆ– ``` ... ```
            pattern = rf"```{re.escape(language.lower())}(?:\s*\n)?(.*?)```"
            m = re.search(pattern, content, re.DOTALL)
            if not m:                      # å†è¯•ä¸€æ¬¡ä¸å¸¦è¯­è¨€æ ‡è®°
                m = re.search(r"```(?:.*?\n)?(.*?)```", content, re.DOTALL)

            if m:
                fixed_code = m.group(1).strip()
                logging.info("âœ… AI å·²æŒ‰è‡ªç„¶è¯­è¨€æ ¼å¼è¿”å›ä»£ç ã€‚")
        
        # å¦‚æœä¸¤ç§æ–¹å¼éƒ½å¤±è´¥ï¼Œè¿”å›åŸå§‹ä»£ç 
        if not fixed_code:
            logging.error("âŒ AI æœªèƒ½ç”Ÿæˆæœ‰æ•ˆçš„ä¿®æ­£ä»£ç ã€‚å°†è¿”å›åŸå§‹ä»£ç ã€‚")
            return buggy_code
        
        return fixed_code
        
    except Exception as e:
        logging.critical(f"âŒ è¯·æ±‚ AI ä¿®å¤ä»£ç æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        logging.error(traceback.format_exc())
        return buggy_code


